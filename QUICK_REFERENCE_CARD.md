# Quick Reference Card

Print this or keep it open while implementing!

---

## ğŸ¯ Your Mission

Transform:
```
âŒ Image â†’ Global Feature â†’ Guess Action
â†’ 
âœ… Image â†’ 3D Objects â†’ Precise Spatial Reasoning â†’ Accurate Action
```

---

## ğŸ“Š Component Checklist

```
IMPLEMENT IN THIS ORDER:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ObjectDetector (YOLOv8)       [ ] 2-3 hrs   â”‚
â”‚    Input: Image  |  Output: Objects with boxes  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. DepthEstimator (MiDaS)        [ ] 2-3 hrs   â”‚
â”‚    Input: Image  |  Output: Depth map (0-1)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3. create_3d_representations()   [ ] 1-2 hrs   â”‚
â”‚    Input: Objects+Depth | Output: (N,7) tensor â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 4. ObjectEncoder                 [ ] 1 hr      â”‚
â”‚    Input: (N,7)  |  Output: (N,256) embeddings â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 5. MultimodalSequenceBuilder     [ ] 2-3 hrs   â”‚
â”‚    Input: All components | Output: (B,~16,256) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 6. Update Data Pipeline          [ ] 2-3 hrs   â”‚
â”‚    Integrate preprocessing + AgentModel update  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 7. Validate & Test               [ ] 2-4 hrs   â”‚
â”‚    Unit tests + Integration tests + Training   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ Code Snippets You'll Need

### ObjectDetector
```python
from ultralytics import YOLO

class ObjectDetector:
    def __init__(self, model_name="yolov8n.pt", device="cpu"):
        self.model = YOLO(model_name)
        self.device = device
        self.model.to(device)
        
    def detect_objects(self, image, conf_threshold=0.5):
        results = self.model(image, conf=conf_threshold, verbose=False)
        # Return list of dicts with 'box', 'center', 'confidence', 'class_id'
```

### DepthEstimator
```python
import torch

class DepthEstimator:
    def __init__(self, model_type="DPT_Large", device="cpu"):
        self.midas = torch.hub.load("intel-isl/MiDaS", model_type)
        self.midas.to(device).eval()
        self.transform = torch.hub.load("intel-isl/MiDaS", "transforms").dpt_transform
        
    def estimate_depth(self, image):
        # Return normalized (0-1) depth map
```

### 3D Fusion
```python
def create_3d_object_representations(objects, depth_map, h, w):
    representations = []
    for obj in objects:
        x1, y1, x2, y2 = obj['box']
        cx, cy = obj['center']
        z = np.mean(depth_map[...])  # Sample depth at bbox
        w_norm = x2 - x1
        h_norm = y2 - y1
        representations.append([cx, cy, z, w_norm, h_norm, 
                               obj['confidence'], obj['class_id']])
    return torch.tensor(representations)
```

### ObjectEncoder
```python
import torch.nn as nn

class ObjectEncoder(nn.Module):
    def __init__(self, in_dim=7, out_dim=256):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Linear(128, out_dim),
            nn.LayerNorm(out_dim),
        )
    
    def forward(self, x):
        return self.net(x)
```

---

## ğŸ“ Files to Create/Modify

| File | Action | Time |
|------|--------|------|
| `src/preprocessing/object_detection.py` | Replace | 2-3h |
| `src/preprocessing/depth_estimation.py` | Replace | 2-3h |
| `src/preprocessing/fusion_utils.py` | Create | 1-2h |
| `src/encoders/object_encoder.py` | Create | 1h |
| `src/fusion/sequence_builder.py` | Create | 2-3h |
| `src/datasets/dataloader.py` | Update | 2-3h |
| **TOTAL** | | **12-17h** |

---

## âœ… Testing Checklist

After each component:

```
ObjectDetector:
  [ ] Finds 3-5 objects in test image
  [ ] Confidence scores 0.5-0.95
  [ ] Boxes normalized (0-1)
  
DepthEstimator:
  [ ] Output range 0-1
  [ ] Reasonable spatial structure
  [ ] No NaN values
  
3D Fusion:
  [ ] Output shape (N, 7)
  [ ] All values 0-1
  [ ] No NaN values
  
ObjectEncoder:
  [ ] Input (N, 7) â†’ Output (N, 256)
  [ ] No gradient issues
  [ ] Trainable weights
  
SequenceBuilder:
  [ ] Constructs (B, seq_len, 256)
  [ ] Handles variable objects
  [ ] No shape mismatches
  
Data Pipeline:
  [ ] collate_fn returns correct dict
  [ ] Preprocessing runs
  [ ] AgentModel forward pass works
  
Training:
  [ ] No NaN loss
  [ ] Loss decreases
  [ ] No OOM errors
```

---

## ğŸ› Debugging Quick Fixes

| Problem | Solution |
|---------|----------|
| No objects detected | Try larger YOLOv8: "yolov8m.pt" |
| Depth values wrong range | Check normalization: `(d - min) / (max - min)` |
| Shape mismatch | Print shapes at each step, check padding |
| Training diverges (NaN) | Reduce learning rate, check coord normalization |
| Out of memory | Reduce batch_size or run on CPU |
| Attention weird | Verify sequence structure is correct |

---

## ğŸ“ˆ Success Metrics by Epoch

```
Epoch 1:   Loss ~2.0   Accuracy ~15%  (Random baseline)
Epoch 5:   Loss ~1.2   Accuracy ~30%  âœ“ Getting better
Epoch 10:  Loss ~0.8   Accuracy ~45%  âœ“ Good progress
Epoch 20:  Loss ~0.5   Accuracy ~65%  âœ“ Converging
Epoch 50:  Loss ~0.4   Accuracy ~75%  âœ“ Excellent
```

---

## ğŸ”‘ Key Insights

| What | Why | How |
|------|-----|-----|
| 3D coords | Model needs to know WHERE | Combine 2D detection + depth |
| Per-object | Can't squeeze all info into 1 vector | Encode each object separately |
| Demo actions | Enable in-context learning | Include action tokens in sequence |
| Frozen encoders | Leverage pre-trained knowledge | Keep BERT & Vision encoder frozen |
| Token sequence | Transformer needs structured input | [instr] + [objs+action]Ã—N + [objs] |

---

## ğŸ“š Documentation Map

```
START HERE!
    â†“
START_HERE.md (5 min)
    â†“
EXECUTIVE_SUMMARY.md (10 min)
    â†“
FILE_IMPLEMENTATION_GUIDE.md (10 min)
    â†“
IMPLEMENTATION_TEMPLATES.py (reference while coding)
    â†“
IMPLEMENTATION_ROADMAP.md (if you need detailed schedule)
    â†“
ARCHITECTURE_VISUAL_SUMMARY.md (if confused)
    â†“
ARCHITECTURE_IMPLEMENTATION_GUIDE.md (deep reference)
    â†“
CURRENT_VS_PROPOSED.md (if questioning design)
```

---

## ğŸ’¡ Pro Tips

1. **Test each component alone first**
   Don't integrate until each works independently

2. **Use debug=True in dataloader**
   Load small debug dataset for quick testing

3. **Print shapes obsessively**
   Most bugs are shape mismatches

4. **Visualize 3D representations**
   Plot bboxes on images to verify correctness

5. **Cache preprocessing results**
   Save object detections + depth to disk for faster iteration

6. **Use Git frequently**
   Commit after each working component

7. **Monitor GPU memory**
   Use `nvidia-smi` to watch memory usage

---

## ğŸš€ Your Timeline

```
DAY 1 (6-8 hours)
â”œâ”€ Morning: Preprocessing pipeline
â”‚  â”œâ”€ ObjectDetector (2h)
â”‚  â”œâ”€ DepthEstimator (2h)
â”‚  â””â”€ 3D Fusion (1h)
â””â”€ Afternoon: Encoding
   â”œâ”€ ObjectEncoder (1h)
   â””â”€ Testing (1-2h)

DAY 2 (6-8 hours)
â”œâ”€ Morning: Sequence building
â”‚  â”œâ”€ SequenceBuilder (2-3h)
â”‚  â””â”€ Testing (1h)
â””â”€ Afternoon: Integration
   â”œâ”€ Update Data Pipeline (2-3h)
   â””â”€ AgentModel integration (1-2h)

DAY 3 (2-4 hours)
â”œâ”€ Unit tests (1h)
â”œâ”€ Integration tests (1h)
â””â”€ Training validation (1-2h)
```

---

## ğŸ“ Quick Help

**Stuck on X?** Check:
- ObjectDetector â†’ `IMPLEMENTATION_TEMPLATES.py` line 10-75
- DepthEstimator â†’ `IMPLEMENTATION_TEMPLATES.py` line 85-155
- 3D Fusion â†’ `IMPLEMENTATION_TEMPLATES.py` line 165-230
- ObjectEncoder â†’ `IMPLEMENTATION_TEMPLATES.py` line 240-275
- SequenceBuilder â†’ `ARCHITECTURE_IMPLEMENTATION_GUIDE.md` Phase 3
- Data Pipeline â†’ `FILE_IMPLEMENTATION_GUIDE.md` Step 6
- Debugging â†’ `IMPLEMENTATION_ROADMAP.md` Debugging section

---

## âœ¨ You've Got This!

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Architecture: âœ… Solid          â”‚
â”‚  Codebase: âœ… 60% Complete       â”‚
â”‚  Documentation: âœ… Comprehensive â”‚
â”‚  Your ability: âœ… More than enoughâ”‚
â”‚                                  â”‚
â”‚  Result: ğŸš€ Success Incoming!    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Next Step:** Open `START_HERE.md` and begin!

**Questions:** Check `DOCUMENTATION_INDEX.md`

**Reference:** Keep this card open while coding!

Good luck! ğŸ‰
