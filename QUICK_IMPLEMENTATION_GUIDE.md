# Quick Implementation Summary

## What You Need to Build

Your proposed architecture has **5 main components**. Here's the status:

### âœ… Component 1: Text Encoder (DONE)
Your `TextEncoder` with BERT is correctly implemented and frozen.

### âœ… Component 2: Vision Encoder (PARTIALLY DONE)
Your `VisionEncoder` uses ResNet18. **Consider upgrading to CLIP's ViT** for better object-level understanding.

### âš ï¸ Component 3: 3D Perception Preprocessing (NEEDS WORK)
**This is your main implementation focus.** Three sub-components:

1. **Object Detection** â†’ Use YOLOv8 instead of OpenCV
   ```python
   from ultralytics import YOLO
   model = YOLO("yolov8n.pt")
   results = model(image)
   ```

2. **Depth Estimation** â†’ Use MiDaS
   ```python
   model = torch.hub.load("intel-isl/MiDaS", "DPT_Large")
   depth = model(image_tensor)
   ```

3. **3D Fusion** â†’ Create object representations
   ```python
   # Combine 2D bboxes + depth â†’ 3D coordinates
   obj_3d = [center_x, center_y, depth, width, height, confidence, class_id]
   ```

### âš ï¸ Component 4: Trainable Policy Network (NEEDS UPDATE)
Your `PolicyTransformer` is good, but the **sequence construction needs work**:

**Current (too simple):**
```
[instruction_embedding, demo_image_embedding, current_image_embedding]
```

**What it should be:**
```
[instruction_embedding,
 demo1_objects, demo1_action,
 demo2_objects, demo2_action,
 ...,
 current_objects]
```

### âœ… Component 5: Output Heads (DONE)
Your 7 classification heads are correctly implemented.

---

## The 4-Phase Implementation Plan

### Phase 1: Build 3D Perception (Days 1-2)
1. Upgrade `ObjectDetector` â†’ YOLOv8
2. Implement `DepthEstimator` â†’ MiDaS  
3. Create `fusion_utils.py` â†’ 3D representations
4. Test each module independently

**Files to create/modify:**
- Modify: `src/preprocessing/object_detection.py`
- Modify: `src/preprocessing/depth_estimation.py`
- Create: `src/preprocessing/fusion_utils.py`

### Phase 2: Data Pipeline (Days 1-2, in parallel)
1. Create `ObjectEncoder` class
2. Update collate function to run preprocessing
3. Handle variable-sized object sets

**Files to create/modify:**
- Create: `src/encoders/object_encoder.py`
- Modify: `src/datasets/dataloader.py` â†’ enhance collate_fn

### Phase 3: Multimodal Sequence (Day 1-2)
1. Create `MultimodalSequenceBuilder` class
2. Properly structure instruction + demos + current scene

**Files to create/modify:**
- Create: `src/fusion/sequence_builder.py`

### Phase 4: Integration (Day 3)
1. Update `AgentModel` to use new components
2. Update training loop if needed
3. Add debugging/visualization

**Files to modify:**
- Modify: `src/datasets/dataloader.py` â†’ AgentModel class
- Modify: `src/training/train.py` â†’ if needed

---

## Most Important Design Decisions

### 1. Which Vision Encoder?
**Current:** ResNet18 â†’ flattened features  
**Recommended:** CLIP ViT â†’ token embeddings per patch

**Why?** ViT naturally produces per-patch embeddings, perfect for per-object encoding.

### 2. How to Handle Variable Object Counts?
**Simplest approach:** Pad all to max, use attention masks  
**Better approach:** Pool or use set operations  
**Start with:** Padding (simpler, good enough for now)

### 3. How to Include Demonstrations?
**Architecture says:** Include object embeddings AND actions from demos  
**Implementation:**
- Extract last valid action from each demo
- Encode objects detected in demo
- Include both in sequence to Transformer
- Transformer learns which demos are relevant

### 4. Action Space Encoding
**Your bins:** [101, 101, 101, 121, 121, 121, 2]  
**Meaning:** First 3 dims have 101 bins (continuous space discretized), next 3 have 121, last is binary (gripper)  
**Include in sequence:** Encode demo actions as additional tokens

---

## Code Structure Recommendation

```
src/
â”œâ”€â”€ encoders/
â”‚   â”œâ”€â”€ text_encoder.py      âœ… Keep as-is
â”‚   â”œâ”€â”€ vision_encoder.py    âœ… Keep, optionally upgrade
â”‚   â””â”€â”€ object_encoder.py    ğŸ†• Create
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ object_detection.py  â™»ï¸ Upgrade to YOLOv8
â”‚   â”œâ”€â”€ depth_estimation.py  â™»ï¸ Implement properly
â”‚   â””â”€â”€ fusion_utils.py      ğŸ†• Create
â”œâ”€â”€ fusion/
â”‚   â”œâ”€â”€ fusion_module.py     ğŸ—‘ï¸ Remove (old approach)
â”‚   â””â”€â”€ sequence_builder.py  ğŸ†• Create
â”œâ”€â”€ policy/
â”‚   â””â”€â”€ policy_transformer.py âœ… Keep as-is
â”œâ”€â”€ heads/
â”‚   â””â”€â”€ output_heads.py      âœ… Keep as-is
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ dataloader.py        â™»ï¸ Update collate_fn
â”‚   â””â”€â”€ transforms.py        âœ… Keep
â””â”€â”€ training/
    â””â”€â”€ train.py             âœ… Keep (minimal changes)
```

---

## Example: What the Data Flow Should Look Like

```
Input: Text instruction + 2 demo videos + current camera frame
        â†“
[Text] â†’ BERT â†’ instr_embedding (768)
        â†“
[Demo Video 1]
  â”œâ”€â†’ Object Detection â†’ 5 objects detected
  â”œâ”€â†’ Depth Estimation â†’ depth map
  â”œâ”€â†’ Fusion â†’ 5 Ã— (center_x, center_y, depth, w, h, conf, cls)
  â”œâ”€â†’ Object Encoder â†’ 5 Ã— 256-dim embeddings
  â””â”€â†’ Extract Action â†’ [x_bin=45, y_bin=32, ..., gripper=1]
        â†“
[Demo Video 2] (same process)
        â†“
[Current Frame]
  â”œâ”€â†’ Object Detection â†’ 3 objects
  â”œâ”€â†’ Depth Estimation â†’ depth map
  â”œâ”€â†’ Fusion â†’ 3 Ã— (center_x, center_y, depth, w, h, conf, cls)
  â””â”€â†’ Object Encoder â†’ 3 Ã— 256-dim embeddings
        â†“
Sequence Builder:
  [instr_embed (256),
   demo1_obj1 (256), demo1_obj2, demo1_obj3, demo1_action (256),
   demo2_obj1 (256), demo2_obj2, ..., demo2_action (256),
   cur_obj1 (256), cur_obj2, cur_obj3]
        â†“
Transformer Policy: seq_lenÃ—256 â†’ 512-dim decision vector
        â†“
Output Heads: 512-dim â†’ [45, 32, ..., 1] (7D action)
```

---

## Recommended Package Additions

```bash
pip install ultralytics  # YOLOv8
pip install timm         # For MiDaS backbone
pip install opencv-python-headless  # Better than cv2
pip install pillow       # Image operations
```

---

## Testing Approach

**Test in isolation first:**
1. Test ObjectDetector on sample images
2. Test DepthEstimator on sample images  
3. Test 3D fusion (visualize to verify)
4. Test ObjectEncoder
5. Test MultimodalSequenceBuilder
6. Test full pipeline integration

**Debugging tools:**
- Visualize 3D bboxes projected onto images
- Check sequence shape at each layer
- Monitor attention patterns in Transformer
- Compare performance: with vs without 3D preprocessing

---

## Success Metrics

After implementation, validate:
- [ ] 3D object representations are spatially accurate
- [ ] Variable object counts handled correctly
- [ ] Sequence length manageable (not exploding)
- [ ] Attention patterns show demonstration relevance
- [ ] Model learns to predict reasonable actions
- [ ] Performance improvement over 2D baseline

